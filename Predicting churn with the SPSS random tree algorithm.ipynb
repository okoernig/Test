{
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.10 with Spark 1.6", 
            "language": "scala", 
            "name": "scala"
        }, 
        "widgets": {
            "version": "1.1.2", 
            "state": {}
        }, 
        "language_info": {
            "name": "scala"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "# Predicting churn with the SPSS random tree algorithm\n\nThis Scala 2.10 notebook shows you how to create a predictive model of churn rate by using IBM SPSS Algorithm on Apache Spark version 1.6. You'll learn how to create an SPSS random tree model by using the IBM SPSS Machine Learning API, and how to view the model with IBM SPSS Model Viewer.\n\nBecause it consists of multiple classification and regression trees (CART), you can use random tree algorithms to generate accurate predictive models and solve complex classification and regression problems. Each tree develops from a bootstrap sample that is produced by resampling the original data points with replacement data. During the resampling phase, the best split variable is selected for each node from a specified smaller number of variables that are drawn randomly from the full set of variables. Each tree grows without pruning and then, during the scoring phase, the random tree algorithm aggregates tree scores by majority voting (for classification) or average (for regression).\n\nIn this notebook, you'll create a model with telecommunications data to predict when its customers will leave for a competitor, so that you can take some action to retain the customer.\n    \nTo get the most out of this notebook, you should have some familiarity with the Scala programming language.\n\n## Contents \nThis notebook contains the following main sections:\n\n1. [Load the Telco Churn data to the cloud data repository.](#overview)\n1. [Prepare the data.](#prepare)\n1. [Configure the RandomTrees model.](#configure) \n1. [View the model.](#view)\n1. [Summary and next steps.](#next)    "
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"overview\"></a>\n## 1. Load the Telco Churn data to the cloud data repository.\nTelco Churn is a hypothetical data file that concerns a telecommunications company's efforts to reduce turnover in its customer base. Each case corresponds to a separate customer and it records various demographic and service usage information. Before you can work with the data, you must use the URL to get the telco.csv and telco_Feb.csv files from the GitHub repository.\n"
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 10, 
            "source": "val link_telco = \"https://raw.githubusercontent.com/AlgorithmDemo/SampleData/master/telco.csv\"\n\nimport sys.process._\nimport java.net.URL\nimport java.io.File\nnew URL(link_telco) #> new File(\"telco.csv\") !!\n\nval link_telco_Feb = \"https://raw.githubusercontent.com/AlgorithmDemo/SampleData/master/telco_Feb.csv\"\n\nimport sys.process._\nimport java.net.URL\nimport java.io.File\nnew URL(link_telco_Feb) #> new File(\"telco_Feb.csv\") !!"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"prepare\"></a>\n## 2. Prepare the data.\n\nAfter uploading the CSV files that contain the data, you must create a SQLContext, put the data from the telco.scv file into a Spark DataFrame, and show the first row in the DataFrame."
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 2, 
            "source": "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\nval dfTelco = sqlContext.\n    read.\n    format(\"com.databricks.spark.csv\").\n    option(\"header\", \"true\").\n    option(\"inferschema\", \"true\").\n    load(\"telco.csv\")\n\ndfTelco.show(1)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "Review the data. Print the schema of the DataFrame to look at what kind of data you have."
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 3, 
            "source": "dfTelco.printSchema"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "Create a DataFrame for the telco_Feb.csv data. You'll use this year's data to build the model, and use the February data for accuracy value."
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 4, 
            "source": "val dfTelcoFeb = sqlContext.\n    read.\n    format(\"com.databricks.spark.csv\").\n    option(\"header\", \"true\").\n    option(\"inferschema\", \"true\").\n    load(\"telco_Feb.csv\")"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"configure\"></a>\n## 3. Configure the RandomTrees model.\n\nBy running this portion of the code, you create the random trees estimator, import the libraries, and set the ordinal and nominal variables. Because no inputFieldList value is set, all fields except the target, frequency, and analysis weight fields are treated as input fields. To make the random tree model build faster, set the number of trees to 10 instead of the default value, which is 100. Finally, you must specify the churn target field. "
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 5, 
            "source": "import com.ibm.spss.ml.classificationandregression.ensemble.RandomTrees\nimport com.ibm.spss.ml.utils.DataFrameImplicits.DataFrameEnrichImplicitsClass\n\nval ordinal = Array(\"ed\")\nval nominal = Array(\"region\",\n     \"marital\",\n     \"retire\",\n     \"gender\",\n     \"tollfree\",\n     \"equip\",\n     \"callcard\",\n     \"wireless\",\"multline\",\n     \"voice\",\"pager\",\"internet\",\"callid\",\"callwait\",\"forward\",\"confer\",\n     \"ebill\",\n     \"custcat\",\n     \"churn\"\n   )\nval srf = RandomTrees().setTargetField(\"churn\").setNumTrees(10)\nval srfModel = srf.fit(dfTelco.setNominalMeasure(nominal,true).setOrdinalMeasure(ordinal,true))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Do the prediction and get your results."
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 6, 
            "source": "val predResult = srfModel.transform(dfTelcoFeb)\nval predResultNew = predResult.withColumn(\"prediction\", predResult(\"prediction\").cast(\"double\")).\n    withColumn(\"churn\", predResult(\"churn\").cast(\"double\"))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "To get the accuracy result, use the Apache Spark **MulticlassClassificationEvaluator** function. Notice that the accuracy is above 90%."
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 7, 
            "source": "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nval evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"churn\").setMetricName(\"precision\")\nval acc_result = evaluator.evaluate(predResultNew)\nprintln(s\"acc_result:$acc_result\")"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"view\"></a>\n## 4. View the model."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "### Show the random trees model result.\nTo see the result, import the IBM SPSS Model Viewer, which you can use to explore different views of the model."
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 8, 
            "source": "import com.ibm.spss.scala.ModelViewer\nkernel.magics.html(ModelViewer.toHTML(srfModel))"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "### Export the XML files (PMML, StatXML) for other detail statistics.\nBy exporting your results to different formats, such as Predictive Model Markup Language (PMML) or statXML format you can share your statistical analyses outside of IBM Data Science Experience."
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "execution_count": 9, 
            "source": "import java.io.{File, PrintWriter}\n\nsrfModel.toPMML(\"randomTrees_pmml.xml\")\nval statXML = srfModel.statXML()\nnew PrintWriter(\"StatXML.xml\") {\n      write(statXML)\n      close\n}"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "<a id=\"next\"></a>\n# Summary and next steps\nYou have created a predictive model of churn rate by using IBM SPSS Algorithm on Apache Spark. Now you can create a different model to compare model evaluations, such as the test of model effects, residuals, and so on. See [SPSS documentation](https://apsportal.ibm.com/docs/content/kc_gen/integrations-gen2.html)."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Authors\n\nWang Zhiyuan and Yu Wenpei are SPSS Algorithm Engineers at IBM."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "Copyright \u00a9 2017 IBM. This notebook and its source code are released under the terms of the MIT License."
        }, 
        {
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "execution_count": null, 
            "source": ""
        }
    ], 
    "nbformat_minor": 0
}